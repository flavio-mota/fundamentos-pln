{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/flavio-mota/fundamentos-pln/blob/master/Introdu%C3%A7%C3%A3o_ao_Processamento_de_Linguagem_Natural.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTkIQCYUkmEH"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/flavio-mota/fundamentos-pln/master/unifei-inpe.png\" align=\"right\" width=\"150\" />\n",
        "\n",
        "<br/>\n",
        "<br/>\n",
        "\n",
        "# <span style=\"color:#336699\">Introdução ao Processamento de Linguagem Natural (PLN)</span>\n",
        "<hr style=\"border:2px solid #0077b9;\">\n",
        "\n",
        "<br/>\n",
        "\n",
        "<div style=\"text-align: center;font-size: 90%;\">\n",
        "    Autores: <br/>\n",
        "    Flávio Belizário da Silva Mota¹ <br/>\n",
        "    Isabela Neves Drummond²\n",
        "    <br/><br/>\n",
        "    ¹Instituto Nacional de Pesquisas Espaciais (INPE) <br/>\n",
        "    ²Universidade Federal de Itajubá (UNIFEI)\n",
        "    <br/>\n",
        "    <br/><br/>\n",
        "    Contato: <a href=\"mailto:flavio.mota@inpe.br\">flavio.mota@inpe.br</a>; <a href=\"mailto:isadrummond@unifei.edu.br\">isadrummond@unifei.edu.br</a>\n",
        "    <br/>\n",
        "</div>\n",
        "\n",
        "<br/>\n",
        "\n",
        "<div style=\"text-align: justify;  margin-left: 25%; margin-right: 25%;\">\n",
        "<b>Objetivo.</b> Esse caderno Jupyter tem como objetivo apresentar alguns conceitos báscios de PLN.\n",
        "</div>\n",
        "\n",
        "<br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0H_sVHX_ky2H"
      },
      "source": [
        "## Objetivos gerais:\n",
        "\n",
        "\n",
        "*   Apresentar noções básicas de sobre o que é o PLN\n",
        "*   Criar funções simples que realizem a limpeza e análise de textos\n",
        "*   Classificação de textos\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXiTEQOG0cGz"
      },
      "source": [
        "## O que é e para que serve um *corpus*?\n",
        "\n",
        "*Corpus* pode ser definido como o conjunto de materiais linguísticos que serão analisados. Para os propósitos do PLN, é necessário que esses materiais estejam na forma de texto e que possam ser lidos por um programa de computador."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xnUfxxAEubs"
      },
      "source": [
        "Vamos carregar um arquivo que contem o texto de diversos e-mails:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xSVbDjHstmNC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "url_spam = 'https://raw.githubusercontent.com/flavio-mota/fundamentos-pln/master/spam_sms.csv'\n",
        "df = pd.read_csv(url_spam,encoding='latin-1')\n",
        "# checando as dimensões do conjunto de dados\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos visualizar um exemplo do conjunto de dados:"
      ],
      "metadata": {
        "id": "Vww_7imzAooM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWaU9Jcw694u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qREnH9AJsjUz"
      },
      "source": [
        "## Tokenização"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6HUA9RUykxr"
      },
      "source": [
        "A *tokenização* é o processo de dividir as sentenças em palavras. Considere a frase:\n",
        "> \"Eu estou lendo um livro.\"\n",
        "\n",
        "A tarefa de *tokenização* tem como objetivo extrair as palavras (ou tokens) dessa sentença, produzindo:\n",
        "> 'Eu', 'estou', 'lendo', 'um', 'livro', ' . '\n",
        "\n",
        "Esse tipo de extração é chamada de 'unigrama', uma vez que separa uma palavra por vez. Entretanto, é possível realizar a extraçã de dois ou três *tokens* por vez. Se são 2 *tokens* por vez, chamamos de **bigramas**. Se são 3, **trigramas**. Dependendo da necessidade, podemos ter **n-gramas**, sendo n um número natural.\n",
        "\n",
        "> **n-gramas** são sequências de n palavras de um texto.\n",
        "\n",
        "No PLN trabalhamos com *tokens* por questão de conveniência, já que temos a tendência de realizar as análises palavra por palavra.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH2TSMCJzPMi"
      },
      "source": [
        "Vamos realizar a tokenização de uma sentença simples. Para isso vamos utilizar um módulo da NLTK que facilita esse processo."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk #importando a biblioteca\n",
        "from nltk import word_tokenize #importando o módulo word_tokenize\n",
        "\n",
        "# Baixar o tokenizador\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "vdIIwgy-9n74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHUQsbdZzl_C"
      },
      "outputs": [],
      "source": [
        "palavras = word_tokenize('Eu estou aprendendo sobre os fundamentos do PLN')\n",
        "palavras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajE03pmgF8oA"
      },
      "source": [
        "## Limpeza do texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpIpXS9wGFo3"
      },
      "source": [
        "Em aplicações de PLN, são avaliadas unidades de texto como sílabas, palavras, frases, entre outras, e não os caracteres do texto. Sendo assim, geralmente são empregadas estratégias de limpeza do texto.\n",
        "\n",
        "Vamos realizar a remoção da pontuação e a conversão de todo o texto para caracteres minúsculos. Podemos definir uma função para que isso possa ser feito sempre que necessário, sem repetição de códigos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfLrC7mwF1KQ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def limpar_texto(texto):\n",
        "    # Converter para minúsculas\n",
        "    texto = texto.lower()\n",
        "\n",
        "    # Remover pontuações e caracteres especiais\n",
        "    texto = re.sub(r'[^\\w\\s]', '', texto)\n",
        "\n",
        "    # Remover números\n",
        "    texto = re.sub(r'\\d+', '', texto)\n",
        "\n",
        "    return texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uz4-rq78Ikjp"
      },
      "outputs": [],
      "source": [
        "texto = \"O Python é uma linguagem de programação poderosa! É ótimo para análise de Dados, IA, ML... :) #Python #Programming\"\n",
        "\n",
        "# Aplicar a função de limpeza ao texto\n",
        "texto_limpo = limpar_texto(texto)\n",
        "print(texto_limpo)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicar a função de limpeza à coluna de texto do DataFrame\n",
        "df['texto_limpo'] = df['texto'].apply(limpar_texto)\n",
        "\n",
        "# Mostrar o DataFrame com os textos originais e os textos limpos\n",
        "df"
      ],
      "metadata": {
        "id": "vhOF25qZ8xlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stopwords"
      ],
      "metadata": {
        "id": "FOZmL_R7-Cfk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utjtgtodL0i0"
      },
      "source": [
        "Outra forma de realizar a limpeza de um *corpus* é através da remoção de **palavras vazias**. Palavras vazias são palavras que podem ser descartadas antes de uma análise pois possuem reduzida contribuição semântica ou nocional. Geralmente são preposições, artigos, conjunções, alguns pronomes e advérbios.\n",
        "\n",
        "A NLTK traz uma lista de 203 palavras vazias para o português. Podemos importa-las usando:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swZW3tOyI8T5"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Baixar a lista de stopwords e o tokenizador\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def remover_stopwords(texto):\n",
        "    # Remover stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = word_tokenize(texto)\n",
        "    texto = ' '.join([word for word in tokens if word not in stop_words])\n",
        "\n",
        "    return texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0Z0QGBLNPZF"
      },
      "source": [
        "Vamos ver um exemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cp19flEINJDV"
      },
      "outputs": [],
      "source": [
        "sentenca = 'Eu estou aprendendo Python. É uma das linguagens de programação mais populares.'\n",
        "print(remover_stopwords(sentenca))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXoJX8rbOCHa"
      },
      "source": [
        "O código acima realizou a remoção das palavras que são consideradas vazias, deixando a sentença estranha para a leitura, mas com mais significado para análises com PLN.\n",
        "\n",
        "Vamos criar uma função que remova as palavras vazias e aplicá-la a obra Dom Casmurro e verificar a mudança no tamanho do *corpus*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sctkW9U5N6ym"
      },
      "outputs": [],
      "source": [
        "# Aplicar a função de limpeza à coluna de texto do DataFrame\n",
        "df['texto_limpo'] = df['texto_limpo'].apply(remover_stopwords)\n",
        "\n",
        "# Mostrar o DataFrame com os textos originais e os textos limpos\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5g0hv8rH-ghr"
      },
      "source": [
        "## Classificando textos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJgkhZd9-nJj"
      },
      "source": [
        "### Bag of Words (BoW)\n",
        "\n",
        "Para implementar o Bag of Words com python, podemos utilizar a [scikit-learn](https://scikit-learn.org/stable/). Especificamente, utilizaremos a CountVectorizer, que é uma classe que permite gerar a representação no formato de matriz dos textos. Utilizaremos também a biblioteca pandas, mas não vamos entrar em detalhes nessa aula. Vamos importar as bibliotecas e criar nosso documento (corpus):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9NKUfAX2JW6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = ['Ciência de Dados é uma interseção entre Artes e Ciência',\n",
        "          'Geralmente, alunos de Artes usam o lado direito do cérebro e estudantes de Ciência o lado esquerdo',\n",
        "          'Destacar-se tanto nas Artes quanto nas Ciências ao mesmo tempo é difícil',\n",
        "          'Processamento de Linguagem Natural faz parte da Ciência de Dados']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1USGhzBuNMJ1"
      },
      "source": [
        "Agora, vamos utilizar o CountVectorizer para criar a representação BoW:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHcLOfjBNBSE"
      },
      "outputs": [],
      "source": [
        "bow = CountVectorizer()\n",
        "bow_df = pd.DataFrame(bow.fit_transform(corpus).todense())\n",
        "bow_df.columns = sorted(bow.vocabulary_)\n",
        "bow_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQYuTskHO0ue"
      },
      "source": [
        "Podemos criar também uma representação BoW somente com os 10 termos mais frequentes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcSKkcw9N0q1"
      },
      "outputs": [],
      "source": [
        "bow_10 = CountVectorizer(max_features=10)\n",
        "bow_df_10 = pd.DataFrame(bow_10.fit_transform(corpus).todense())\n",
        "bow_df_10.columns = sorted(bow_10.vocabulary_)\n",
        "bow_df_10.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MUZ3XGBWkpf"
      },
      "source": [
        "### TF-IDF\n",
        "\n",
        "Assim como na representação BoW, utilizaremos a scikit-learn para gerar o TF-IDF, através da classe TfidfVectorizer. Vamos empregar o mesmo corpus do exemplo anterior:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gK8nOguLPRaJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "corpus = ['Ciência de Dados é uma interseção entre Artes e Ciência',\n",
        "          'Geralmente, alunos de Artes usam o lado direito do cérebro e estudantes de Ciência o lado esquerdo',\n",
        "          'Destacar-se tanto nas Artes quanto nas Ciências ao mesmo tempo é difícil',\n",
        "          'Processamento de Linguagem Natural faz parte da Ciência de Dados']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyvuPxv6XP9n"
      },
      "source": [
        "Vamos gerar a representação TF-IDF:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mad7CHWyXNPe"
      },
      "outputs": [],
      "source": [
        "tfidf = TfidfVectorizer()\n",
        "tfidf_df = pd.DataFrame(tfidf.fit_transform(corpus).todense())\n",
        "tfidf_df.columns = sorted(tfidf.vocabulary_)\n",
        "tfidf_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxPkaRLCYe1P"
      },
      "source": [
        "### Classificando: Spam\n",
        "\n",
        "Vamos classificar alguns textos como spam e não spam. Para isso, iremos empregar as técnicas que vimos anteriormente e um classificador. Ainda não vimos classificadores, mas não se preocupe. O intuito aqui é apenas ver uma das aplicações do Processamento de Linguagem Natural."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6GbJwmjbB5n"
      },
      "source": [
        "Vamos visualizar a distribuição das classes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pUi-a5IZtff"
      },
      "outputs": [],
      "source": [
        "df.classe.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMjVG_BzbnuP"
      },
      "source": [
        "Para representar as classes como valores numéricos de forma que o modelo de classificação consiga entender, vamos codificar cada valor (ham e spam) como 0 e 1, respectivamente, e gerar uma nova coluna chamada rotulo_num:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2q_OWNlbUpP"
      },
      "outputs": [],
      "source": [
        "df['classe_num']=df.classe.map({\"ham\":0,\"spam\":1})\n",
        "# checando a conversão\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYE2LSpxfzdc"
      },
      "source": [
        "Vamos criar agora as variáveis x e y para armazenar os textos e os rótulos. X e y são nomes de variáveis muito empregadas no desenvolvimento de modelos de aprendizagem com python, sendo que X armazena os dados e y os rótulos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kk4r7rjNftfy"
      },
      "outputs": [],
      "source": [
        "x = df.texto_limpo\n",
        "y = df.classe_num"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6DsJPOTghJB"
      },
      "source": [
        "É importante separar os dados em um conjunto de treinamento e um de testes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuS9h1mrgHXP"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,random_state=42)\n",
        "\n",
        "print(\"Tamanho do conjunto de treino:\", x_train.shape)\n",
        "print(\"Tamanho do conjunto de teste:\", x_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4XIaN9kg44N"
      },
      "source": [
        "Agora, iremos aplicar a vetorização dos dados com a técnica BoW:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOR82DJRgwdY"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vect = CountVectorizer()\n",
        "\n",
        "x_train_dtm = vect.fit_transform(x_train)\n",
        "x_test_dtm = vect.transform(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqXAARr0hUwY"
      },
      "source": [
        "Classificaremos os dados com o algoritmo Naïve Bayes. Esse algoritmo é um dos mais empregados em tarefas de classificação para aplicações de Processamento de Linguagem Natural:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtDeymXehJoX"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "nb = MultinomialNB()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJQT9knqhqsJ"
      },
      "source": [
        "Treinando o modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_zTefI4hoH4"
      },
      "outputs": [],
      "source": [
        "%time nb.fit(x_train_dtm,y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9rAE_Zkhw4P"
      },
      "source": [
        "Vamos gerar uma predição:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDItd2GFhu94"
      },
      "outputs": [],
      "source": [
        "y_pred_class = nb.predict(x_test_dtm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1Otrk6Bh4NA"
      },
      "source": [
        "Calculando a acurácia do modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F97P8NAsh2DH"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "metrics.accuracy_score(y_test,y_pred_class)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZK8KyEEWh-ag"
      },
      "source": [
        "O modelo alcançou uma acurácia de 98%. Mas será que ele está realmente acertando a classe que importa, ou seja, os spams?\n",
        "\n",
        "Podemos utilizar uma matriz de confusão para verificar isso:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52tmkffHh7Wn"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_class, labels=nb.classes_)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
        "                              display_labels=['normal', 'spam'])\n",
        "disp.plot()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKaLZWl2iXt4"
      },
      "source": [
        "A interpretação da matriz nos diz o seguinte:\n",
        "* x - eram spam e foram classificados como spam\n",
        "* x - não eram spam, mas foram classicados como spam\n",
        "* x - eram spam, mas não foram classificados como spam\n",
        "* x - não eram spam, e não foram classificados como spam"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}